{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0561116d",
   "metadata": {},
   "source": [
    "# Report : Project of Basics of Mobile Robotics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8acb39",
   "metadata": {},
   "source": [
    "Group members :\n",
    "\n",
    "- Cirillo Thomas\n",
    "- Muller Nathan\n",
    "- Sahin Ali Fuat\n",
    "- Vinet Pierre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- sous parties table of contents\n",
    "- link table of content\n",
    "- import libraries\n",
    "INTRODUCTION\n",
    "- image environment \n",
    "- we didn't use any libraries for the algorithms (other than numpy etc... )\n",
    "- description of aruc markers (maybe in the image processing part)\n",
    "I IMAGE PROCESSING\n",
    "- implement images processing (image of the obstacles from the camera, detection markers, image of the processed environment )\n",
    "II PATH FINDING\n",
    "- more images? \n",
    "III GLOBAL NAVIGATION\n",
    "- image copmaring a-star path to real pd implementation\n",
    "IV LOCAL OBSTACLE\n",
    "- plot of the thymio coming back to the path\n",
    "V KALMAN FILTER\n",
    "- plot image with camera and without\n",
    "CONCLUSION\n",
    "- conclusion sur les solutions apporter et limitations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1. Introduction](#introduction)\n",
    "- [2. Image Processing](#image-processing)\n",
    "- [3. Path Finding](#path-finding)\n",
    "- [4. Global Navigation](#global-navigation)\n",
    "- [5. Local Obstacle Avoidance](#local-obstacle-avoidance)\n",
    "- [6. Kalman Filter](#kalman-filter)\n",
    "- [7. Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9791982",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\environment-description.png\"alt=\"environment\" id=\"img_1.1\" width=\"1000\" >\n",
    "  <p style=\"text-align: center;\">Image 1.1: Environment</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617551",
   "metadata": {},
   "source": [
    "Our environment consist of a white surface of 1 by 1 meters, with Aruco markers on the different corners to be able to identify them using the webcam given to us. \n",
    "\n",
    "We first decided to use a white surface with a black grid but then decided to go with a fully white surface instead because we use a binary threshold to detect our obstacles which would then interpret the grid as an obstacle. To correct this we would had needed to specifically extract the grid to create a mask out of it and substract it to our image in order to not detect it as an obstacle but we would not need this if we used a simple white surface. We also decided that the obstacle were not just squared one and therefor the use of a grid made less sense. \n",
    "\n",
    "The start and the goal are represented as Aruco markers as well. The obstacle used for the global navigation are black objects because as said before we use a binary threshold to detect them and blakc works the best for this situation, and the one used for the local avoidance are white objects because since they have the same color as the surface they are not detected with our image processing program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84bbed",
   "metadata": {},
   "source": [
    "### 1.2 Object-Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose an object-oriented approach because it offers a structured and intuitive way to encapsulate data and functions into objects, providing a clear and organized framework for our project. This choice allows us to efficiently manage relationships between various components, and easily modify or add new functionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Code Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73394082",
   "metadata": {},
   "source": [
    "The code is structured into the following files :\n",
    "\n",
    "- Image Processing (`ImageProcessing.py`)\n",
    "- Kalman Filter (`KalmanFilter.py`)\n",
    "- PathFinding (`PathFinding.py`)\n",
    "- Global Navigation (`Navigation.py`)\n",
    "- Local Avoidance (`ThymioFunctions.py`)\n",
    "- Connection To Thymio (`Thymio.py`) \n",
    "- The Main Code (`main.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to detect 4 aruco markers present at the 4 corners of the environment using the webcam given to use for the project. The reason we chose aruco markers is that we wanted markers tht the webcam could detect robustly. We tried using QR codes but we found out afterward that Aruco markers were more easily and consistently detected by the webcam than QR codes. Those were more sensible to light differences causing them to be less effectively detected than Aruco markers who seem more robust against those changes. Once the 4 corners are detected and the image has been correctly warped, it detects and updates the position of the robots and of the goal, thresholding the image to find the global obstacles and creating a grid for the pathinding algorithm to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the library OpenCV, we connect to the webcam and keep on taking frames to analyze while the camera is connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Import the OpenCV library to use the different modules\n",
    "import cv2\n",
    "\n",
    "#Connect to the webcam \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#As long as the webcam is connected, the frames are store in the image variable\n",
    "while cap.isOpened():\n",
    "\t\tret, image = cap.read()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the camera is connected, it will warp the image giving by the webcam to make the image fit the screen. To do so, it checks the transform_start variable : \n",
    "<ul>\n",
    "   <li> if set to <b>True</b>, it calls the functions detecting Aruco markers which return the different coordinates of the different markers\n",
    "   \n",
    "```python\n",
    "    if transform_start:\n",
    "                (coords, image) = aruco_read(image, transform=True, start=True)\n",
    "```\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aruco_read function (given below) takes as argument the image that will be processed as well as two boolean variables (transform and start). \n",
    "It checks for Aruco markers on the image. An if branchement make sure that if all the wanted markers are not visible, it returns an empty array instead of coordinate as well as the original image.\n",
    "Finally, if all the different markers are detected, it put their different coordinate in a list, draw 4 lines around it to make it more visible on the image and draw a red dot in its center. After it has gone through all the markers, it return the list containing the coordinates and the image with the drawn line and dots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def aruco_read(image, transform, start):\n",
    "\tARUCO_DICT = {\n",
    "\t\t\"DICT_4X4_50\": cv2.aruco.DICT_4X4_50,\n",
    "\t\t\"DICT_4X4_100\": cv2.aruco.DICT_4X4_100,\n",
    "\t\t\"DICT_4X4_250\": cv2.aruco.DICT_4X4_250,\n",
    "\t\t\"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n",
    "\t\t\"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,\n",
    "\t\t\"DICT_5X5_100\": cv2.aruco.DICT_5X5_100,\n",
    "\t\t\"DICT_5X5_250\": cv2.aruco.DICT_5X5_250,\n",
    "\t\t\"DICT_5X5_1000\": cv2.aruco.DICT_5X5_1000,\n",
    "\t\t\"DICT_6X6_50\": cv2.aruco.DICT_6X6_50,\n",
    "\t\t\"DICT_6X6_100\": cv2.aruco.DICT_6X6_100,\n",
    "\t\t\"DICT_6X6_250\": cv2.aruco.DICT_6X6_250,\n",
    "\t\t\"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n",
    "\t\t\"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,\n",
    "\t\t\"DICT_7X7_100\": cv2.aruco.DICT_7X7_100,\n",
    "\t\t\"DICT_7X7_250\": cv2.aruco.DICT_7X7_250,\n",
    "\t\t\"DICT_7X7_1000\": cv2.aruco.DICT_7X7_1000,\n",
    "\t\t\"DICT_ARUCO_ORIGINAL\": cv2.aruco.DICT_ARUCO_ORIGINAL,\n",
    "\t\t\"DICT_APRILTAG_16h5\": cv2.aruco.DICT_APRILTAG_16h5,\n",
    "\t\t\"DICT_APRILTAG_25h9\": cv2.aruco.DICT_APRILTAG_25h9,\n",
    "\t\t\"DICT_APRILTAG_36h10\": cv2.aruco.DICT_APRILTAG_36h10,\n",
    "\t\t\"DICT_APRILTAG_36h11\": cv2.aruco.DICT_APRILTAG_36h11\n",
    "\t}\n",
    "\n",
    "\taruco_dict = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[\"DICT_4X4_50\"])\n",
    "\taruco_params = cv2.aruco.DetectorParameters()\n",
    "\taruco_det = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)\n",
    "\n",
    "    #Detect if markers are recognized and stock the different variables in corners, ids and rejected\n",
    "\t(corners, ids, rejected) = aruco_det.detectMarkers(image)\n",
    "    \n",
    "    #Check if the ids detected from the markers (if any) are the correct one\n",
    "\tif ids is None:\n",
    "\t\treturn None, image\n",
    "\telif transform and 0 in ids and 1 in ids and 2 in ids and 3 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and start and 4 in ids and 5 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and not start and (4 in ids or 5 in ids):\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\treturn None, image\n",
    "    \n",
    "    #If all the correct markers are detected, put all their coordinates in a list and draw their contour and center\n",
    "\tcoord_list = []\n",
    "\tids = ids.flatten()\n",
    "    \n",
    "    #Goes through all the coordinates and ids of the detected markers\n",
    "\tfor (marker_corners, marker_ids) in zip(corners, ids):\n",
    "\t\tcorners = marker_corners.reshape((4,2))\n",
    "\t\t(topLeft, topRight, bottomRight, bottomLeft) = corners\n",
    "        \n",
    "        #Put all the coordinates and ids in a list\n",
    "\t\tcoord_list.append({'ID': marker_ids, 'POS': np.squeeze(corners)})\t\t\t\n",
    "\t\n",
    "        #Draw the contour of the marker\n",
    "\t\ttopRight = (int(topRight[0]), int(topRight[1]))\n",
    "\t\tbottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "\t\tbottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "\t\ttopLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    "\t\tcv2.line(image, topLeft, topRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, topRight, bottomRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomRight, bottomLeft, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomLeft, topLeft, (0, 255, 0), 2)\n",
    "\t\t\n",
    "        #Draw the center of it\n",
    "\t\tcX = int((topLeft[0] + bottomRight[0]) / 2.0)\n",
    "\t\tcY = int((topLeft[1] + bottomRight[1]) / 2.0)\n",
    "\t\tcv2.circle(image, (cX, cY), 4, (0, 0, 255), -1)\n",
    "\t\t\t\n",
    "\treturn coord_list, image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this function has return the different coordinates and image to the main, it checks if if the coordinates vector is empty or not and if ny coordinates are given, it switch transform_start to <b>False</b>:\n",
    "\n",
    "```python\n",
    "if coords is not None:\n",
    "    transform_start = False\n",
    "continue         \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Finally, if the tranform_start variable is set to <b>False</b>, it calls the function change_perspective whoe goal is to warp the desired part of the image to make it fit the screen\n",
    "</ul>\n",
    "\n",
    "```python\n",
    "else:\n",
    "\timage = change_perpective(image, coords, size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function (given below) takes as arguments the image we want to warp, the different coordinates and ids of the Aruco markers we detected previously and the size of the desired output. Going through all the markers detected, an if branchement checks if the procesed marker is on of the corners and put it as entry coordinates for the warp. After this, it put all the given entry coordinates in a corner variable (transforming them in float by the same occasion). It does the same for the output coordinates putting them in a dim variable (also transforming them in float).\n",
    "The getPerspectiveTransform and warpPerspective functions from Open CV are called.\n",
    "The way the warping works is that it takes input and output coordinates and warp the given image so that the input coordinate A is now situated at the output coordinate A (therefor, in our code it puts the different corners at the point (0,0), (1000,0), (0,1000), (1000,1000); 1000 being the value for size[0] and size[1])\n",
    "Finally, the function return the warped image to the main."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def change_perpective(image, coords, size):\n",
    "\ttransform_coords = np.zeros((4,2))\n",
    "    \n",
    "    #Goes through all the detected markers to assignated corners coordinate as input coordinates\n",
    "\tfor el in coords:\n",
    "\t\tif el.get('ID') == 0:\n",
    "\t\t\ttransform_coords[0] = el.get('POS')[2]\n",
    "\t\telif el.get('ID') == 1:\n",
    "\t\t\ttransform_coords[1] = el.get('POS')[3]\n",
    "\t\telif el.get('ID') == 2:\n",
    "\t\t\ttransform_coords[2] = el.get('POS')[1]\n",
    "\t\telif el.get('ID') == 3:\n",
    "\t\t\ttransform_coords[3] = el.get('POS')[0]\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "    \n",
    "    #Declare corners (input coordinates) and dim (output coordinates)\n",
    "\tcorners = np.float32(transform_coords)\n",
    "\tdim = np.float32([[0,0],[size[0],0],[0,size[1]],[size[0],size[1]]])\n",
    "\n",
    "    #Warp the image\n",
    "\tperspective = cv2.getPerspectiveTransform(corners, dim)\n",
    "\timage = cv2.warpPerspective(image, perspective, (size[0], size[1]))\n",
    "\t\n",
    "\treturn image\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def calculate_centroid(coords):\n",
    "    x_center = 0\n",
    "    y_center = 0\n",
    "    for i in range(4):\n",
    "        x_center += coords[i][0]/4\n",
    "        y_center += coords[i][1]/4\n",
    "    return [x_center, y_center]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect the different obstacles once all of the previously stated elements are detected, a threshold is applied to the image. This threshold allows to better identify the global obstacles, the environment being white and the obstacles being black (the function is givenbelow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def image_threshold(image, border_size, robot_coords, goal_coords):\n",
    "    \n",
    "    #Convert to grayscale\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Reduce image noise\n",
    "\tblur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    \n",
    "    #Threshold image to better detect obstacle\n",
    "\t_, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\t\n",
    "    #Remove robot and goal Aruco marker from contour detections\n",
    "\tthresh = delete_aruco(thresh, robot_coords)\n",
    "\tthresh = delete_aruco(thresh, goal_coords)\n",
    "\n",
    "\tkernel = np.ones((5, 5), np.uint8) \n",
    "\tthresh = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    #Detect remaining obstacles\n",
    "\tcontours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcv2.drawContours(thresh, contours, -1, 0, border_size)\n",
    "\t\n",
    "\treturn thresh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the image is first converted from BRG to grayscale in order to accuentuate the contrast between the elements. A gaussian blur is then applied, his goal being to reduce the noise of the image to improve the obstacle detection. Finally, a threshold is applied to the image. We first only used a binary threshold but the resulted algorithm was very dependent on the light conditions and we therefor added an Otsu threshold which allowed to reduce the impact lightning had on the detection.\n",
    "\n",
    "The obstacle detection uses contour detection checking for any polygon it can find on the image. However, the Aruco markers used to represent the robot as well as the goal as also polygons, so, the delete_aruco function (given below) is called, in order to ignore those two.\n",
    "\n",
    "```python\n",
    "def delete_aruco(thresh, pos):\n",
    "\tmax_p = [0, 0]\n",
    "\tmin_p = [float('inf'), float('inf')]\n",
    "\tfor point in pos:\n",
    "\t\tif point[0] > max_p[0]:\n",
    "\t\t\tmax_p[0] = int(point[0])\n",
    "\t\tif point[0] < min_p[0]:\n",
    "\t\t\tmin_p[0] = int(point[0])\n",
    "\t\tif point[1] > max_p[1]:\n",
    "\t\t\tmax_p[1] = int(point[1])\n",
    "\t\tif point[1] < min_p[1]:\n",
    "\t\t\tmin_p[1] = int(point[1])\n",
    "\tthresh = cv2.rectangle(thresh, min_p, max_p, 255, -1)\n",
    "\treturn thresh\n",
    "```\n",
    "\n",
    "Then, we use the findContours and drawContours functions from Open CV to detect the global obstacles.\n",
    "\n",
    "The function returning the thresholded image containing only the obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the thresholded image containing the obstacles, a grid is created using the define_grid function (given below). This grid being used by the pathfinding algorithm.\n",
    "\n",
    "```python\n",
    "def define_grid(size, spacing, thresh):\n",
    "\tbackground = np.zeros([size[0], size[1], 1], dtype=np.uint8)\n",
    "\n",
    "\tvert = size[0]//spacing\n",
    "\thorz = size[1]//spacing\n",
    "\tv_blank = (size[0] % spacing)//2 + spacing\n",
    "\th_blank = (size[1] % spacing)//2 + spacing\n",
    "\n",
    "\tcoord_init = (v_blank, h_blank)\n",
    "\tgrid = []\n",
    "\tcoord = []\n",
    "\n",
    "\tfor h in range(horz-1):\n",
    "\t\tcoord_init = (h_blank + h*spacing, v_blank)\n",
    "\t\tfor v in range(vert-1):\n",
    "\t\t\tif thresh.T[coord_init] == 255:\n",
    "\t\t\t\tbackground = cv2.circle(background, coord_init, 5, 255, -1)\n",
    "\t\t\t\tgrid.append((h,v))\n",
    "\t\t\t\tcoord.append(coord_init)\n",
    "\t\t\tcoord_init = (coord_init[0], coord_init[1] + spacing)\n",
    "\t\t\t\n",
    "\treturn grid, coord, background\n",
    "```\n",
    "\n",
    "It first created a zero filled matrix of the size of the image. An arbitrary spacing is creating so that the different point of the grid are separated by the same distance. A double for loop then goes through the entire matrix and check the value of the thresholded image obtained before at the position we are verifying. If the value is 255 (white), a white dot is put at the position meaning it is free of any obstacles and the coordinates are put in a list containing all the obstacle free coordinate.\n",
    "\n",
    "The function then returns the grid, the completed background as well as the coordinates where there are no obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def find_pos(pos, grid, coord):\n",
    "\tgrid_c = None\n",
    "\tdist = float('inf')\n",
    "\tfor point in coord:\n",
    "\t\ttemp = euclidean_distance(point, pos)\n",
    "\t\tif temp < dist:\n",
    "\t\t\tgrid_c = grid[coord.index(point)]\n",
    "\t\t\tdist = temp\n",
    "\n",
    "\treturn grid_c\n",
    "```\n",
    "\n",
    "Finally, the function find_pos (given above) is called in the main, returning the corresponding position of the start and the goal on the grid we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Path Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to find the shortest path between two points in a grid-like structure while minimizing the total cost associated with the path. <br /><br />\n",
    "We chose to use the A* Algorithm because it it garanteed to find the shortest path and it is more efficient than other algorithms in terms of both time and memory usage.<br/><br />\n",
    "We use this algorithm to find the shortest path from the robot initial position to the robot targeted position. At this point of the project we are taking into account only the global obstacles visible from the camera, and not the local ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\a-star-path-example.jpg\"alt=\"path\" id=\"img_3.1\" width=\"700\" >\n",
    "  <p style=\"text-align: center;\">Image 3.1: Example of the shortest path from the Thymio robot (top left) to the target (bottom right) using A* Algorithm.</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Node class initializes each node with its coordinates (coord) and the goal node's coordinates (goal).\n",
    "- The g value is the cost from the start node to the current node, initialized to infinity.\n",
    "- The _h value is the heuristic estimate  of the cost to reach the goal from the current node.\n",
    "- The _f value is the sum of g and _h, used to determine the node's priority in the pathfinding process.\n",
    "- The set_cost and set_parent methods are used to update a node's cost and its parent in the path.\n",
    "- The _get_dist method calculates the heuristic distance (Manhattan distance) between two nodes, considering both straight and diagonal movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The A_star class is initialized with a grid and a set of camera coordinates.\n",
    "- We chose 8 possible movements directions from a node.\n",
    "- The _path_constructor method reconstructs the path from the start to the goal node, traversing backward from the goal.\n",
    "- The _is_traversible method checks if a coordinate is within the grid and thus accessible.\n",
    "- The find_path method implements the core A* algorithm. It uses a priority queue (heapq) to efficiently select the next node to explore. The method returns the constructed path from start to goal if available, or an empty list if no path is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to make the robot follow the desired path with the help of a controller. We compare two type of conroller :\n",
    "- A *PD controller* \n",
    "- An *Astolfi controller*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 PD coontroler\n",
    "\n",
    "**proportional-derivative (PD) controller** is a type of feedback control system commonly used i control theory. It has two main components: *proportional (P) control* and *derivative (D) control*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for a PD controller :  \n",
    "$$c(t) = K_c(e(t) + T_d \\frac{de}{dt}) + C$$\n",
    "\n",
    "\n",
    "Therefore we defined a class **PD_controller** that contains : \n",
    "-  Initalisation : P (constant of proportionality of error signal), D (constant of proportionality of derivative of the error signal), time_step.\n",
    "-  A control method that take the error and the previous error to calculate the controller output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Astolfi controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To smoothly modulate the forward and rotational velocities, we can use an **Astolfi controller**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\coord_astolfi.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 4.2.1: Polar and global coordinates of the Thymio</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the figure above (<a href=\"#img_4.2.1\">Image 3.2.1</a>), we can define the global coordinates of the robots by using his local coordinates by using those equations : \n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\polar_astolfi.png\" alt=\"Alt Text\" id=\"img_4.2.2\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 4.2.2: Equations for polar coordinates</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "The target is $\\alpha,\\rho,\\beta$ to be equal to (0,0,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this last set of equations, we can output the linear and angular velocities for the Thymio : \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\final_astolfi.png\" alt=\"Alt Text\" id=\"img_3.2.3\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 4.2.3: Output of the controler</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code this is all achived in the **astolfi_controller** class. We have :\n",
    "- The initialisation method defined $k_{\\rho}, k_{\\alpha}, k_{\\beta}$\n",
    "- The **_calculate_parameter** method calculate $\\rho,\\alpha, \\beta$ with the help of the goal and the postion of the robot.\n",
    "- The **control** method use the **_calculate_parameter** method to return the linear and angular velocities $(v, \\omega)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Other functions for the navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many functions were develloped to implement the navigation part. Here is a recap of the most important functions  : \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **calculate_centroid** function computes the centroid of the four corners of an aruco marker. The resulting centroid is returned as a 2-dimensional vector: $\\text{{centroid}} = [(x_{\\text{{center}}}),(y_{\\text{{center}}})]$, where $x_{\\text{{center}}} = \\frac{x_1 + x_2 + x_3 + x_4}{4}$ and $y_{\\text{{center}}} = \\frac{y_1 + y_2 + y_3 + y_4}{4}$. The $x_{\\text{{center}}}$ and $y_{\\text{{center}}}$ are represented as an integer because we are working with pixels so we cannot use floating point numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **calculate_centroid** function computes the centroid of the four corners of an aruco marker. The resulting centroid is returned as a 2-dimensional vector: $\\text{{centroid}} = [(x_{\\text{{center}}}),(y_{\\text{{center}}})]$, where $x_{\\text{{center}}} = \\frac{x_1 + x_2 + x_3 + x_4}{4}$ and $y_{\\text{{center}}} = \\frac{y_1 + y_2 + y_3 + y_4}{4}$. The $x_{\\text{{center}}}$ and $y_{\\text{{center}}}$ are represented as an integer because we are working with pixels so we cannot use floating point numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **calculate_error** function calculates the error between the robot's position and a given path. The input parameters include the path and the current coordinates of the robot. The function iterates through each segment of the path, determining the projection of the robot's position onto the path. If the robot moves beyond the current path segment, the function calculates the error based on the distance between the robot and the nearest endpoint of the path segment. The sign of the error indicates whether the robot is to the right or left of the path, as determined by the **rel_orientation** function. The resulting error is returned, considering the sign and the Euclidean distance between the robot's centroid and the projected position on the path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Extended Kalman Filter, an extension of the classic Kalman Filter for nonlinear systems, is an ideal tool for this scenario. It helps in predicting the future state of the robot by estimating its current position and movement, even in the absence of direct positional inputs from the external camera. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Initial Approach\n",
    "\n",
    "Initially, our approach to state estimation in the Thymio robot navigation project relied on using motor speed and camera inputs as the primary data sources for the Kalman filter. This setup seemed promising as it directly utilized the robot's inherent capabilities to infer its movement and orientation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Motor Speed Noise\n",
    "\n",
    "However, we soon encountered a significant challenge. The speed sensor in the Thymio robot exhibited a high degree of noise. This noise severely distorted the state estimation. For example, at a true speed of 50 mm/s, our estimates varied wildly between 20 and 80 mm/s. Such a wide margin of error was unacceptable for precise navigation and pose estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 State Space Equations for Differential Drive Robot\n",
    "\n",
    "To address this, we shifted our focus to a more robust solution. We decided to predict the state of the robot based on the state space equations of a differential drive robot. This method relies less on noisy sensor data and more on the physical and theoretical model of the robot's movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extended_kalman:\n",
    "\tdef __init__(self, Q, H, R, sampling_rate):\n",
    "\t\tself._Q = Q\n",
    "\t\tself._H = H\n",
    "\t\tself._R = R\n",
    "\t\tself._sampling_rate = sampling_rate        \n",
    "\n",
    "\tdef predict(self, x_init, P_init, u):\n",
    "\t\tF = np.eye(3) #+ np.array([[0, 0, -u[0]*np.sin(x_init[2])], [0, 0, u[0]*np.cos(x_init[2])], [0, 0, 0]])*self._sampling_rate\n",
    "\t\tG = np.array([[np.cos(x_init[2]), 0], [np.sin(x_init[2]), 0], [0, -1]])*self._sampling_rate\n",
    "\t\tx_est = np.dot(F, x_init) + np.dot(G, u)\n",
    "\t\tP_est = np.dot(np.dot(F, P_init), F.T) + self._Q\n",
    "\t\t\n",
    "\t\treturn x_est, P_est\n",
    "\t\n",
    "\tdef correct(self, x_est, P_est, y):\n",
    "\t\ti = y - np.dot(self._H, x_est)\n",
    "\t\tj = np.eye(len(self._H))\n",
    "\t\tS = np.dot(self._H, np.dot(P_est, self._H.T)) + self._R\n",
    "\t\tKn = np.dot(np.dot(P_est, self._H.T), np.linalg.inv(S))\n",
    "\t\t\n",
    "\t\tx_final = x_est + np.dot(Kn, i)\n",
    "\t\tP_final = np.dot(j, np.dot(P_est, j.T)) + np.dot(Kn, np.dot(self._R, Kn.T))\n",
    "\t\t\n",
    "\t\treturn x_final, P_final\n",
    "\t\n",
    "\n",
    "\tdef run(self, x_init, P_init, u, y):\n",
    "\t\tx_est, P_est = self.predict(x_init, P_init, u)\n",
    "\t\treturn self.correct(x_est, P_est, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "The constructor __init__ initializes the filter with necessary parameters:\n",
    "\n",
    "- `Q` : Process noise covariance matrix\n",
    "- `H` : Measurement matrix\n",
    "- `R` : Measurement noise covariance matrix\n",
    "- `sampling_rate` : The rate at which the measurements are sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Step\n",
    "\n",
    "The predict method implements the prediction step of the EKF.\n",
    "\n",
    "- `x_init`: Initial state estimate\n",
    "- `P_init` : Initial covariance estimate\n",
    "- `u` : Control input vector\n",
    "In this step, we calculate the estimated state x_est and the covariance of the estimate P_est using the state transition model and control input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction Step\n",
    "\n",
    "The correct method implements the correction step.\n",
    "\n",
    "- `x_est`: Estimated state from the prediction step\n",
    "- `P_est`: Estimated covariance from the prediction step\n",
    "- `y` : Measurement vector\n",
    "\n",
    "This step updates the estimated state and covariance based on the measurement y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Filter\n",
    "The run method combines the predict and correct steps to process a single measurement.\n",
    "\n",
    "`x_init`, `P_init`, `u`, `y`: Inputs for the prediction and correction steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
