{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0561116d",
   "metadata": {},
   "source": [
    "# Report : Project of Basics of Mobile Robotics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8acb39",
   "metadata": {},
   "source": [
    "Group members :\n",
    "\n",
    "- Cirillo Thomas\n",
    "- Muller Nathan\n",
    "- Sahin Ali Fuat\n",
    "- Vinet Pierre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a287e",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- sous parties table of contents\n",
    "- link table of content\n",
    "- import libraries\n",
    "INTRODUCTION\n",
    "- image environment \n",
    "- we didn't use any libraries for the algorithms (other than numpy etc... )\n",
    "- description of aruc markers (maybe in the image processing part)\n",
    "I IMAGE PROCESSING\n",
    "- implement images processing (avant warp, image of the obstacles from the camera, detection markers, image of the processed environment )\n",
    "II PATH FINDING\n",
    "III GLOBAL NAVIGATION\n",
    "- image copmaring a-star path to real pd implementation\n",
    "IV LOCAL OBSTACLE\n",
    "- plot of the thymio coming back to the path\n",
    "V KALMAN FILTER\n",
    "MAIN?\n",
    "\n",
    "CONCLUSION\n",
    "- conclusion sur les solutions apporter et limitations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d013521",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1. Introduction](#introduction)\n",
    "    - [1.1 The Environment](#the-environment)\n",
    "    - [1.2 Object-Oriented Programming](#object-oriented-programming)\n",
    "    - [1.3 Code Structure](#code-structure)\n",
    "    - [1.4 Libraries Import](#libraries-import)\n",
    "- [2. Image Processing](#image-processing)\n",
    "    - [2.1 Getting Image from webcam](#getting-image-from-webcam)\n",
    "    - [2.2 Detection of Aruco markers](#detection-of-aruco-markers)\n",
    "    - [2.3 Warping of the image](#warping-of-the-image)\n",
    "    - [2.4 Obstacles detection](#obstacles-detection)\n",
    "    - [2.5 Grid creation](#grid-creation)\n",
    "- [3. Path Finding](#path-finding)\n",
    "    - [3.1 Node Definition](#node-definition)\n",
    "    - [3.2 A* Algorithm](#a*-algorithm)\n",
    "- [4. Global Navigation](#global-navigation)\n",
    "    - [4.1 PD controller](#pd-controller)\n",
    "    - [4.2 Astolfi controller](#astolfi-controller)\n",
    "    - [4.3 Other functions for the navigation](#other-functions-for-the-navigation)\n",
    "- [5. Local Obstacle Avoidance](#local-obstacle-avoidance)\n",
    "- [6. Kalman Filter](#kalman-filter)\n",
    "    - [6.1 Initial Approach](#initial-approach)\n",
    "    - [6.2 Motor Speed Noise](#motor-speed-noise)\n",
    "    - [6.3 State Space Equations for Differential Drive Robot](#state-space-equations-for-differential-drive-robots)\n",
    "    - [6.4 Extended Kalman Filter](#extended-kalman-filter)\n",
    "        - [6.4.1 Initialization](#initialization)\n",
    "        - [6.4.2 Prediction Step](#prediction-step)\n",
    "        - [6.4.3 Correction Step](#correction-step)\n",
    "        - [6.4.4 Running the Filter](#running-the-filter)\n",
    "- [7. Project Execution](#project-execution)\n",
    "- [8. Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9791982",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b5f71",
   "metadata": {},
   "source": [
    "### 1.1 The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502070dd",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\environment-description.png\"alt=\"environment\" id=\"img_1.1\" width=\"1000\" >\n",
    "  <p style=\"text-align: center;\">Image 1.1: Environment</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617551",
   "metadata": {},
   "source": [
    "Our environment consist of a white surface of 1 by 1 meters, with Aruco markers on the different corners to be able to identify them using the webcam given to us. \n",
    "\n",
    "We first decided to use a white surface with a black grid but then decided to go with a fully white surface instead because we use a binary threshold to detect our obstacles which would then interpret the grid as an obstacle. To correct this we would had needed to specifically extract the grid to create a mask out of it and substract it to our image in order to not detect it as an obstacle but we would not need this if we used a simple white surface. We also decided that the obstacle were not just squared one and therefor the use of a grid made less sense. \n",
    "\n",
    "The start and the goal are represented as Aruco markers as well. The obstacle used for the global navigation are black objects because as said before we use a binary threshold to detect them and blakc works the best for this situation, and the one used for the local avoidance are white objects because since they have the same color as the surface they are not detected with our image processing program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84bbed",
   "metadata": {},
   "source": [
    "### 1.2 Object-Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ce178",
   "metadata": {},
   "source": [
    "We chose an object-oriented approach because it offers a structured and intuitive way to encapsulate data and functions into objects, providing a clear and organized framework for our project. This choice allows us to efficiently manage relationships between various components, and easily modify or add new functionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3f18c",
   "metadata": {},
   "source": [
    "### 1.3 Code Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73394082",
   "metadata": {},
   "source": [
    "The code is structured into the following files :\n",
    "\n",
    "- Image Processing (`ImageProcessing.py`)\n",
    "- Kalman Filter (`KalmanFilter.py`)\n",
    "- PathFinding (`PathFinding.py`)\n",
    "- Global Navigation (`Navigation.py`)\n",
    "- Local Avoidance (`ThymioFunctions.py`)\n",
    "- Connection To Thymio (`Thymio.py`) \n",
    "- The Main Code (`main.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8751f",
   "metadata": {},
   "source": [
    "### 1.4 Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd12f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/38/d2/3e8c13ffc37ca5ebc6f382b242b44acb43eb489042e1728407ac3904e72f/opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata\n",
      "  Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pierr\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pierr\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\pierr\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\pierr\\anaconda3\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (6.27.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (8.17.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (5.13.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\pierr\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.8)\n",
      "Requirement already satisfied: psutil in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.6)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (4.0.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (306)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\pierr\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python tqdm numpy ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e833c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Thymio import Thymio\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde61ee",
   "metadata": {},
   "source": [
    "## 2. Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a1bc9",
   "metadata": {},
   "source": [
    "The goal of this section is to detect 4 aruco markers present at the 4 corners of the environment using the webcam given to use for the project. The reason we chose aruco markers is that we wanted markers tht the webcam could detect robustly. We tried using QR codes but we found out afterward that Aruco markers were more easily and consistently detected by the webcam than QR codes. Those were more sensible to light differences causing them to be less effectively detected than Aruco markers who seem more robust against those changes. Once the 4 corners are detected and the image has been correctly warped, it detects and updates the position of the robots and of the goal, thresholding the image to find the global obstacles and creating a grid for the pathinding algorithm to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be32ce",
   "metadata": {},
   "source": [
    "### 2.1 Getting Image from webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac5118",
   "metadata": {},
   "source": [
    "Using the OpenCV library, we connect to the webcam using cv2.VideoCapture() and analyze the frames while the camera is connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c476d9",
   "metadata": {},
   "source": [
    "``` python\n",
    "#Connect to the webcam \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#As long as the webcam is connected, the frames are store in the image variable\n",
    "while cap.isOpened():\n",
    "\tret, image = cap.read()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04408a",
   "metadata": {},
   "source": [
    "### 2.2 Detection of Aruco markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911c2e3",
   "metadata": {},
   "source": [
    "Once the camera is connected, it will warp the image giving by the webcam to make the image fit the screen. To do so, it checks the transform_start variable : \n",
    "<ul>\n",
    "   <li> if set to <b>True</b>, it calls the functions detecting Aruco markers which return the different coordinates of the different markers\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d05e3",
   "metadata": {},
   "source": [
    "This aruco_read function (given below) takes as argument the image that will be processed and two boolean. \n",
    "It checks for Aruco markers on the image. When all the desired markers are detected, the coordinates and ids are written in a list used to draw their contours and center.\n",
    "\n",
    "It then returns this list and the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d753c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aruco_read(image, transform, start):\n",
    "\tARUCO_DICT = {\n",
    "\t\t\"DICT_4X4_50\": cv2.aruco.DICT_4X4_50,\n",
    "\t\t\"DICT_4X4_100\": cv2.aruco.DICT_4X4_100,\n",
    "\t\t\"DICT_4X4_250\": cv2.aruco.DICT_4X4_250,\n",
    "\t\t\"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n",
    "\t\t\"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,\n",
    "\t\t\"DICT_5X5_100\": cv2.aruco.DICT_5X5_100,\n",
    "\t\t\"DICT_5X5_250\": cv2.aruco.DICT_5X5_250,\n",
    "\t\t\"DICT_5X5_1000\": cv2.aruco.DICT_5X5_1000,\n",
    "\t\t\"DICT_6X6_50\": cv2.aruco.DICT_6X6_50,\n",
    "\t\t\"DICT_6X6_100\": cv2.aruco.DICT_6X6_100,\n",
    "\t\t\"DICT_6X6_250\": cv2.aruco.DICT_6X6_250,\n",
    "\t\t\"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n",
    "\t\t\"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,\n",
    "\t\t\"DICT_7X7_100\": cv2.aruco.DICT_7X7_100,\n",
    "\t\t\"DICT_7X7_250\": cv2.aruco.DICT_7X7_250,\n",
    "\t\t\"DICT_7X7_1000\": cv2.aruco.DICT_7X7_1000,\n",
    "\t\t\"DICT_ARUCO_ORIGINAL\": cv2.aruco.DICT_ARUCO_ORIGINAL,\n",
    "\t\t\"DICT_APRILTAG_16h5\": cv2.aruco.DICT_APRILTAG_16h5,\n",
    "\t\t\"DICT_APRILTAG_25h9\": cv2.aruco.DICT_APRILTAG_25h9,\n",
    "\t\t\"DICT_APRILTAG_36h10\": cv2.aruco.DICT_APRILTAG_36h10,\n",
    "\t\t\"DICT_APRILTAG_36h11\": cv2.aruco.DICT_APRILTAG_36h11\n",
    "\t}\n",
    "\n",
    "\taruco_dict = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[\"DICT_4X4_50\"])\n",
    "\taruco_params = cv2.aruco.DetectorParameters()\n",
    "\taruco_det = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)\n",
    "\n",
    "    #Detect if markers are recognized and stock the different variables in corners, ids and rejected\n",
    "\t(corners, ids, rejected) = aruco_det.detectMarkers(image)\n",
    "    \n",
    "    #Check if the ids detected from the markers (if any) are the correct one\n",
    "\tif ids is None:\n",
    "\t\treturn None, image\n",
    "\telif transform and 0 in ids and 1 in ids and 2 in ids and 3 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and start and 4 in ids and 5 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and not start and (4 in ids or 5 in ids):\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\treturn None, image\n",
    "    \n",
    "    #If all the correct markers are detected, put all their coordinates in a list and draw their contour and center\n",
    "\tcoord_list = []\n",
    "\tids = ids.flatten()\n",
    "    \n",
    "    #Goes through all the coordinates and ids of the detected markers\n",
    "\tfor (marker_corners, marker_ids) in zip(corners, ids):\n",
    "\t\tcorners = marker_corners.reshape((4,2))\n",
    "\t\t(topLeft, topRight, bottomRight, bottomLeft) = corners\n",
    "        \n",
    "        #Put all the coordinates and ids in a list\n",
    "\t\tcoord_list.append({'ID': marker_ids, 'POS': np.squeeze(corners)})\t\t\t\n",
    "\t\n",
    "        #Draw the contour of the marker\n",
    "\t\ttopRight = (int(topRight[0]), int(topRight[1]))\n",
    "\t\tbottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "\t\tbottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "\t\ttopLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    "\t\tcv2.line(image, topLeft, topRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, topRight, bottomRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomRight, bottomLeft, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomLeft, topLeft, (0, 255, 0), 2)\n",
    "\t\t\n",
    "        #Draw the center of it\n",
    "\t\tcX = int((topLeft[0] + bottomRight[0]) / 2.0)\n",
    "\t\tcY = int((topLeft[1] + bottomRight[1]) / 2.0)\n",
    "\t\tcv2.circle(image, (cX, cY), 4, (0, 0, 255), -1)\n",
    "\t\t\t\n",
    "\treturn coord_list, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2dc74",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Finally, if the tranform_start variable is set to <b>False</b>, it calls the function change_perspective whose goal is to warp the desired part of the image to make it fit the screen\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c032a9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\beforeTransform.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 2.2.1: Image as seen from the webcam with detection of Aruco markers</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f0d12",
   "metadata": {},
   "source": [
    "### 2.3 Warping of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646baee3",
   "metadata": {},
   "source": [
    "This function (given below) takes as arguments the image we want to warp, the different coordinates and ids of the Aruco markers and the size of the desired output.\n",
    "\n",
    "We decided to warp the image because it reduce the amount of unused data the program processes.\n",
    "\n",
    "The function store the coordinates of the different corners as input coordinates for the warp. It then declare output coordinates based on the size variable declared before.\n",
    "\n",
    "The way the warping works is that it takes input and output coordinates and warp the given image so that the input coordinate A is now situated at the output coordinate A (therefor, in our code it puts the different corners at the point (0,0), (1000,0), (0,1000), (1000,1000); 1000 being the value for size[0] and size[1])\n",
    "\n",
    "Finally, the function return the warped image to the main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6dc80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_perpective(image, coords, size):\n",
    "\ttransform_coords = np.zeros((4,2))\n",
    "    \n",
    "    #Goes through all the detected markers to assignated corners coordinate as input coordinates\n",
    "\tfor el in coords:\n",
    "\t\tif el.get('ID') == 0:\n",
    "\t\t\ttransform_coords[0] = el.get('POS')[2]\n",
    "\t\telif el.get('ID') == 1:\n",
    "\t\t\ttransform_coords[1] = el.get('POS')[3]\n",
    "\t\telif el.get('ID') == 2:\n",
    "\t\t\ttransform_coords[2] = el.get('POS')[1]\n",
    "\t\telif el.get('ID') == 3:\n",
    "\t\t\ttransform_coords[3] = el.get('POS')[0]\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "    \n",
    "    #Declare corners (input coordinates) and dim (output coordinates)\n",
    "\tcorners = np.float32(transform_coords)\n",
    "\tdim = np.float32([[0,0],[size[0],0],[0,size[1]],[size[0],size[1]]])\n",
    "\n",
    "    #Warp the image\n",
    "\tperspective = cv2.getPerspectiveTransform(corners, dim)\n",
    "\timage = cv2.warpPerspective(image, perspective, (size[0], size[1]))\n",
    "\t\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448bc1f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\afterTransform.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 2.3.1: Warped image</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df881e5",
   "metadata": {},
   "source": [
    "Once the image has been warped, the same detecting process is done to detect the position of the robot and the goal, both detected with Aruco markers (the detection being made if the movement is starting and also if it has already being launched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe1bd8",
   "metadata": {},
   "source": [
    "### 2.4 Obstacles detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f500f63",
   "metadata": {},
   "source": [
    "In order to detect the different obstacles once all of the previously stated elements are detected, a threshold is applied to the image. This threshold allows to better identify the global obstacles, the environment being white and the obstacles being black (the function is givenbelow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d21ef8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_aruco(thresh, pos):\n",
    "\tmax_p = [0, 0]\n",
    "\tmin_p = [float('inf'), float('inf')]\n",
    "\tfor point in pos:\n",
    "\t\tif point[0] > max_p[0]:\n",
    "\t\t\tmax_p[0] = int(point[0])\n",
    "\t\tif point[0] < min_p[0]:\n",
    "\t\t\tmin_p[0] = int(point[0])\n",
    "\t\tif point[1] > max_p[1]:\n",
    "\t\t\tmax_p[1] = int(point[1])\n",
    "\t\tif point[1] < min_p[1]:\n",
    "\t\t\tmin_p[1] = int(point[1])\n",
    "\tthresh = cv2.rectangle(thresh, min_p, max_p, 255, -1)\n",
    "\treturn thresh\n",
    "\n",
    "def image_threshold(image, border_size, robot_coords, goal_coords):\n",
    "    \n",
    "    #Convert to grayscale\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Reduce image noise\n",
    "\tblur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    \n",
    "    #Threshold image to better detect obstacle\n",
    "\t_, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\t\n",
    "    #Remove robot and goal Aruco marker from contour detections\n",
    "\tthresh = delete_aruco(thresh, robot_coords)\n",
    "\tthresh = delete_aruco(thresh, goal_coords)\n",
    "\n",
    "\tkernel = np.ones((5, 5), np.uint8) \n",
    "\n",
    "\t#Dilatation of th image to reduce even more the noise\n",
    "\tthresh = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    #Detect remaining obstacles\n",
    "\tcontours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #Draw obstacles contours\n",
    "\tcv2.drawContours(thresh, contours, -1, 0, border_size)\n",
    "\t\n",
    "\treturn thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f1942",
   "metadata": {},
   "source": [
    "The image is first converted from BRG to grayscale to accuentuate the contrast between the elements. A gaussian blur is then applied to reduce the noise of the image improving the obstacle detection. Finally, a threshold is applied to the image. We first only used a binary threshold because it was the most efficient one in our case (black obstacles with white surface) but the sensibility to light was to high so we added an Otsu threshold allowing to reduce the impact lightning had on the detection.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\firstThreshold.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 2.4.1: Image after threshold</p>\n",
    "</div>\n",
    "\n",
    "The obstacles, that are polygons are detected with OpenCV. \n",
    "However, the Aruco markers used to represent the robot as well as the goal as also polygons, so, the delete_aruco function (given above) has to be called to not detect those two.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\deleteArucoMarkers.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 2.4.2: Suppression of Aruco markers</p>\n",
    "</div>\n",
    "\n",
    "Also, the make sure we will not be detecting small pixels or noise still present in the image, we dilate the image to prevent it from happening.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\eliminateNoise.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "      <p style=\"text-align: center;\">Image 2.4.3: Image after dilatation (noise elimination)</p>\n",
    "</div>\n",
    "\n",
    "Contours are then drawn around the obstacles at a certain bordersize to make sure the Thymio will completely avoid them\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\drawContours.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 2.4.4: Image with obstacles and their contours</p>\n",
    "</div>\n",
    "\n",
    "The function then returns the thresholded image containing the obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a47258",
   "metadata": {},
   "source": [
    "### 2.5 Grid creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f9cfa",
   "metadata": {},
   "source": [
    "Finally, using the thresholded image containing the obstacles, a grid is created using the define_grid function (given below). This grid being used by the pathfinding algorithm, the spacing between each dot representing 80mm in rality. This spacing can be modified but this settings has be kept because it produces satisfying results.\n",
    "\n",
    "To create the grid, an empty matrix the size of the image is created and creates a dot everywhere there are no obstacles from the thresholded image.\n",
    "\n",
    "The function then returns the completed grid as well as the coordinates where there are no obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e593321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_grid(size, spacing, thresh):\n",
    "\tbackground = np.zeros([size[0], size[1], 1], dtype=np.uint8)\n",
    "\n",
    "\tvert = size[0]//spacing\n",
    "\thorz = size[1]//spacing\n",
    "\tv_blank = (size[0] % spacing)//2 + spacing\n",
    "\th_blank = (size[1] % spacing)//2 + spacing\n",
    "\n",
    "\tcoord_init = (v_blank, h_blank)\n",
    "\tgrid = []\n",
    "\tcoord = []\n",
    "\n",
    "\tfor h in range(horz-1):\n",
    "\t\tcoord_init = (h_blank + h*spacing, v_blank)\n",
    "\t\tfor v in range(vert-1):\n",
    "\t\t\tif thresh.T[coord_init] == 255:\n",
    "\t\t\t\tbackground = cv2.circle(background, coord_init, 5, 255, -1)\n",
    "\t\t\t\tgrid.append((h,v))\n",
    "\t\t\t\tcoord.append(coord_init)\n",
    "\t\t\tcoord_init = (coord_init[0], coord_init[1] + spacing)\n",
    "\t\t\t\n",
    "\treturn grid, coord, background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d0295",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\grid.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 2.5.1: Generated grid from the previous image</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e4654",
   "metadata": {},
   "source": [
    "Finally, a find_pos function is called in the main, returning the corresponding position of the start and the goal on the grid we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822bec1",
   "metadata": {},
   "source": [
    "## 3. Path Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552df2ab",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\a-star-path-example.jpg\"alt=\"path\" id=\"img_3.1\" width=\"700\" >\n",
    "  <p style=\"text-align: center;\">Image 3.1: Example of the shortest path from the Thymio robot (top left) to the target (bottom right) using A* Algorithm.</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370238f",
   "metadata": {},
   "source": [
    "The goal of this section is to find the shortest path between two points in a grid-like structure while minimizing the total cost associated with the path. <br /><br />\n",
    "We chose to use the A* Algorithm because it it garanteed to find the shortest path and it is more efficient than other algorithms in terms of both time and memory usage.<br/><br />\n",
    "We use this algorithm to find the shortest path from the robot initial position to the robot targeted position. At this point of the project we are taking into account only the global obstacles visible from the camera, and not the local ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf78036",
   "metadata": {},
   "source": [
    "### 3.1 Node Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, coord, goal):\n",
    "        self.coord = coord\n",
    "        self.g = float('inf')\n",
    "        self._h = self._get_dist(self.coord, goal)\n",
    "        self._f = self.g + self._h\n",
    "        self.parent = None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.coord == other.coord\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self._f < other._f\n",
    "        \n",
    "    def set_cost(self, g):\n",
    "        if g < self.g:\n",
    "            self.g = g\n",
    "            self._f = self.g + self._h\n",
    "            \n",
    "    def set_parent(self, parent):\n",
    "        if parent is not None:\n",
    "            self.parent = parent\n",
    "        \n",
    "    def _get_dist(self, node_A, node_B):\n",
    "        dist_x = math.fabs(node_A[0] - node_B[0])\n",
    "        dist_y = math.fabs(node_A[1] - node_B[1])\n",
    "        \n",
    "        if dist_x > dist_y:\n",
    "            dist = math.sqrt(2)*dist_y + (dist_x - dist_y)\n",
    "        else:\n",
    "            dist = math.sqrt(2)*dist_x + (dist_y - dist_x)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83493646",
   "metadata": {},
   "source": [
    "The Node class initializes each node with its coordinates (coord) and the goal node's coordinates (goal).\n",
    "- The g value is the cost from the start node to the current node, initialized to infinity.\n",
    "- The _h value is the heuristic estimate  of the cost to reach the goal from the current node.\n",
    "- The _f value is the sum of g and _h, used to determine the node's priority in the pathfinding process.\n",
    "- The set_cost and set_parent methods are used to update a node's cost and its parent in the path.\n",
    "- The _get_dist method calculates the heuristic distance (Manhattan distance) between two nodes, considering both straight and diagonal movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c821a",
   "metadata": {},
   "source": [
    "### 3.2 A* Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dc1f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A_star:\n",
    "    \n",
    "    def __init__(self, grid, camera_coords):\n",
    "        self._grid = grid\n",
    "        self._camera_coords = camera_coords\n",
    "    \n",
    "    def _get_movements(self, movements):\n",
    "        if movements == \"8n\":\n",
    "            s2 = math.sqrt(2)\n",
    "            return [(1, 0, 1.0),\n",
    "                    (0, 1, 1.0),\n",
    "                    (-1, 0, 1.0),\n",
    "                    (0, -1, 1.0),\n",
    "                    (1, 1, s2),\n",
    "                    (-1, 1, s2),\n",
    "                    (-1, -1, s2),\n",
    "                    (1, -1, s2)]\n",
    "        else:\n",
    "            return [(1, 0, 1.0),\n",
    "                    (0, 1, 1.0),\n",
    "                    (-1, 0, 1.0),\n",
    "                    (0, -1, 1.0)]\n",
    "    \n",
    "    def _path_constructor(self, final_node):\n",
    "        path = []\n",
    "        construct = final_node\n",
    "        \n",
    "        while construct is not None:\n",
    "            path.append(self._camera_coords[self._grid.index(construct.coord)])\n",
    "            construct = construct.parent\n",
    "            \n",
    "        return path\n",
    "    \n",
    "    def _is_traversible(self, coord):\n",
    "        if coord in self._grid:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def find_path(self, start, goal, movements = \"8n\"):\n",
    "        start_node = Node(start, goal)\n",
    "        start_node.set_cost(0)\n",
    "        open_set = [start_node]\n",
    "        closed_set = []\n",
    "        heapq.heapify(open_set)\n",
    "        \n",
    "        while open_set:\n",
    "            current = heapq.heappop(open_set)\n",
    "            if current in closed_set:\n",
    "                continue\n",
    "            closed_set.append(current)\n",
    "            if current.coord == goal:\n",
    "                return self._path_constructor(current)\n",
    "            \n",
    "            for dx, dy, dg in self._get_movements(movements):\n",
    "                coord = (current.coord[0] + dx, current.coord[1] + dy)\n",
    "                if self._is_traversible(coord):\n",
    "                    neighbour = Node(coord, goal)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if neighbour in closed_set:\n",
    "                    continue\n",
    "            \n",
    "                if neighbour in open_set:\n",
    "                    for obj in open_set:\n",
    "                        if obj == neighbour:\n",
    "                            del neighbour\n",
    "                            neighbour = obj\n",
    "                else:\n",
    "                    neighbour.set_cost(current.g + dg)\n",
    "                    neighbour.set_parent(current)\n",
    "                    heapq.heappush(open_set, neighbour)\n",
    "\n",
    "                if neighbour.g > current.g + dg:\n",
    "                    neighbour.set_cost(current.g + dg)\n",
    "                    neighbour.set_parent(current)\n",
    "                    \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441bc98",
   "metadata": {},
   "source": [
    "The A_star class is initialized with a grid and a set of camera coordinates.\n",
    "- We chose 8 possible movements directions from a node.\n",
    "- The _path_constructor method reconstructs the path from the start to the goal node, traversing backward from the goal.\n",
    "- The _is_traversible method checks if a coordinate is within the grid and thus accessible.\n",
    "- The find_path method implements the core A* algorithm. It uses a priority queue (heapq) to efficiently select the next node to explore. The method returns the constructed path from start to goal if available, or an empty list if no path is found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e13725",
   "metadata": {},
   "source": [
    "## 4. Global Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7d8f0",
   "metadata": {},
   "source": [
    "The goal of this section is to make the robot follow the desired path with the help of a controller. We compare two type of conroller :\n",
    "- A *PD controller* \n",
    "- An *Astolfi controller*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ede5e",
   "metadata": {},
   "source": [
    "### 4.1 PD coontroler\n",
    "\n",
    "**proportional-derivative (PD) controller** is a type of feedback control system commonly used i control theory. It has two main components: *proportional (P) control* and *derivative (D) control*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95874b5c",
   "metadata": {},
   "source": [
    "The equation for a PD controller :  \n",
    "$$c(t) = K_c(e(t) + T_d \\frac{de}{dt}) + C$$\n",
    "\n",
    "\n",
    "Therefore we defined a class **PD_controller** that contains : \n",
    "-  Initalisation : P (constant of proportionality of error signal), D (constant of proportionality of derivative of the error signal), time_step.\n",
    "-  A control method that take the error and the previous error to calculate the controller output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb2b7f",
   "metadata": {},
   "source": [
    "### 4.2 Astolfi controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee41033",
   "metadata": {},
   "source": [
    "To smoothly modulate the forward and rotational velocities, we can use an **Astolfi controller**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d595a6e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\coord_astolfi.png\"alt=\"Alt Text\" id=\"img_3.2.1\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 4.2.1: Polar and global coordinates of the Thymio</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d1c99",
   "metadata": {},
   "source": [
    "As shown in the figure above (<a href=\"#img_4.2.1\">Image 3.2.1</a>), we can define the global coordinates of the robots by using his local coordinates by using those equations : \n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\polar_astolfi.png\" alt=\"Alt Text\" id=\"img_4.2.2\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 4.2.2: Equations for polar coordinates</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "The target is $\\alpha,\\rho,\\beta$ to be equal to (0,0,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2661b7",
   "metadata": {},
   "source": [
    "With this last set of equations, we can output the linear and angular velocities for the Thymio : \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images\\final_astolfi.png\" alt=\"Alt Text\" id=\"img_3.2.3\" width=\"300\">\n",
    "  <p style=\"text-align: center;\">Image 4.2.3: Output of the controler</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc398f46",
   "metadata": {},
   "source": [
    "In our code this is all achived in the **astolfi_controller** class. We have :\n",
    "- The initialisation method defined $k_{\\rho}, k_{\\alpha}, k_{\\beta}$\n",
    "- The **_calculate_parameter** method calculate $\\rho,\\alpha, \\beta$ with the help of the goal and the postion of the robot.\n",
    "- The **control** method use the **_calculate_parameter** method to return the linear and angular velocities $(v, \\omega)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd5125",
   "metadata": {},
   "source": [
    "### 4.3 Other functions for the navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b9360",
   "metadata": {},
   "source": [
    "Many functions were develloped to implement the navigation part. Here is a recap of the most important functions  : \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc18803",
   "metadata": {},
   "source": [
    "- The **calculate_centroid** function computes the centroid of the four corners of an aruco marker. The resulting centroid is returned as a 2-dimensional vector: $\\text{{centroid}} = [(x_{\\text{{center}}}),(y_{\\text{{center}}})]$, where $x_{\\text{{center}}} = \\frac{x_1 + x_2 + x_3 + x_4}{4}$ and $y_{\\text{{center}}} = \\frac{y_1 + y_2 + y_3 + y_4}{4}$. The $x_{\\text{{center}}}$ and $y_{\\text{{center}}}$ are represented as an integer because we are working with pixels so we cannot use floating point numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2c52f",
   "metadata": {},
   "source": [
    "- The **calculate_centroid** function computes the centroid of the four corners of an aruco marker. The resulting centroid is returned as a 2-dimensional vector: $\\text{{centroid}} = [(x_{\\text{{center}}}),(y_{\\text{{center}}})]$, where $x_{\\text{{center}}} = \\frac{x_1 + x_2 + x_3 + x_4}{4}$ and $y_{\\text{{center}}} = \\frac{y_1 + y_2 + y_3 + y_4}{4}$. The $x_{\\text{{center}}}$ and $y_{\\text{{center}}}$ are represented as an integer because we are working with pixels so we cannot use floating point numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fff609",
   "metadata": {},
   "source": [
    "- The **calculate_error** function calculates the error between the robot's position and a given path. The input parameters include the path and the current coordinates of the robot. The function iterates through each segment of the path, determining the projection of the robot's position onto the path. If the robot moves beyond the current path segment, the function calculates the error based on the distance between the robot and the nearest endpoint of the path segment. The sign of the error indicates whether the robot is to the right or left of the path, as determined by the **rel_orientation** function. The resulting error is returned, considering the sign and the Euclidean distance between the robot's centroid and the projected position on the path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd806e",
   "metadata": {},
   "source": [
    "## 5. Local Avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c815d",
   "metadata": {},
   "source": [
    "The goal of this section is to allow the robot, in case an obstacle is not detected by the webcam and image processing section of the code, to avoid this \"invisible\" obstacle and to succesfully return on the optimal path that was computed with A star once there are no more obstacle detected. Our local avoidance was done using a form of ANN (artificial neural network), this type of local avoidance being chosen as it allowed the robot to move quite smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dacc39c",
   "metadata": {},
   "source": [
    "In our main function, we first check that a path has been made to not check for local avoidance for no reasons. If it is good, an other check will be done to verify if we are walking down the path or trying to go back to it. A final check is made to see if we are far enough from the goal, as there is no point in making a local avoidance if we are already on the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05468dbc",
   "metadata": {},
   "source": [
    "The local_nn function (given below) takes as argument th, a Thymio object as well as v, the linear velocity variable, as well as w, the rottional velocity variable. \n",
    "\n",
    "As it is based on ANN, weights for the horizontal sensors for the left and right motors are declared, the given values were find to give the best result for the used Thymio.\n",
    "\n",
    "Those weifhts are then multiplied by the sensors values.\n",
    "\n",
    "Finally, we subtract the right speed from the left one so that we can write it in the w variable, responsible for the rotation of the robot. The fact that we subrtract one from the other is the if the robot turn of a certain value to the left and of a certain value to the right, the total rotation will be the difference between the two, which is what we do here.\n",
    "\n",
    "The function then returns v and w that will be used to update the motors in the main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb963325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_nn(th, v, w):\n",
    "    \n",
    "    #Weights\n",
    "\twl = [1, 2, 3, -2, -1, 1, -1]\n",
    "\twr = [-1, -2, -3, 2, 1, -1, 1]\n",
    "    \n",
    "\tscale = 40\n",
    "\tL = 95 #mm\n",
    "    \n",
    "    #Sensors value multiplied by the different weights\n",
    "\tspeedL = np.dot(th[\"prox.horizontal\"], wl)//scale\n",
    "\tspeedR = np.dot(th[\"prox.horizontal\"], wr)//scale\n",
    "\n",
    "    #Total rotation of the Thymio\n",
    "\tw = w + (speedR - speedL)/(2*L)\n",
    "\n",
    "\treturn v, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d787a3",
   "metadata": {},
   "source": [
    "## 6. Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb64d9",
   "metadata": {},
   "source": [
    "The Extended Kalman Filter, an extension of the classic Kalman Filter for nonlinear systems, is an ideal tool for this scenario. It helps in predicting the future state of the robot by estimating its current position and movement, even in the absence of direct positional inputs from the external camera. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3faee",
   "metadata": {},
   "source": [
    "### 6.1 Initial Approach\n",
    "\n",
    "Initially, our approach to state estimation in the Thymio robot navigation project relied on using motor speed and camera inputs as the primary data sources for the Kalman filter. This setup seemed promising as it directly utilized the robot's inherent capabilities to infer its movement and orientation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3385b32",
   "metadata": {},
   "source": [
    "### 6.2 Motor Speed Noise\n",
    "\n",
    "However, we soon encountered a significant challenge. The speed sensor in the Thymio robot exhibited a high degree of noise. This noise severely distorted the state estimation. For example, at a true speed of 50 mm/s, our estimates varied wildly between 20 and 80 mm/s. Such a wide margin of error was unacceptable for precise navigation and pose estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad18982",
   "metadata": {},
   "source": [
    "### 6.3 State Space Equations for Differential Drive Robot\n",
    "\n",
    "To address this, we shifted our focus to a more robust solution. We decided to predict the state of the robot based on the state space equations of a differential drive robot. This method relies less on noisy sensor data and more on the physical and theoretical model of the robot's movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75c0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class extended_kalman:\n",
    "\tdef __init__(self, Q, H, R, sampling_rate):\n",
    "\t\tself._Q = Q\n",
    "\t\tself._H = H\n",
    "\t\tself._R = R\n",
    "\t\tself._sampling_rate = sampling_rate        \n",
    "\n",
    "\tdef predict(self, x_init, P_init, u):\n",
    "\t\tF = np.eye(3) #+ np.array([[0, 0, -u[0]*np.sin(x_init[2])], [0, 0, u[0]*np.cos(x_init[2])], [0, 0, 0]])*self._sampling_rate\n",
    "\t\tG = np.array([[np.cos(x_init[2]), 0], [np.sin(x_init[2]), 0], [0, -1]])*self._sampling_rate\n",
    "\t\tx_est = np.dot(F, x_init) + np.dot(G, u)\n",
    "\t\tP_est = np.dot(np.dot(F, P_init), F.T) + self._Q\n",
    "\t\t\n",
    "\t\treturn x_est, P_est\n",
    "\t\n",
    "\tdef correct(self, x_est, P_est, y):\n",
    "\t\ti = y - np.dot(self._H, x_est)\n",
    "\t\tj = np.eye(len(self._H))\n",
    "\t\tS = np.dot(self._H, np.dot(P_est, self._H.T)) + self._R\n",
    "\t\tKn = np.dot(np.dot(P_est, self._H.T), np.linalg.inv(S))\n",
    "\t\t\n",
    "\t\tx_final = x_est + np.dot(Kn, i)\n",
    "\t\tP_final = np.dot(j, np.dot(P_est, j.T)) + np.dot(Kn, np.dot(self._R, Kn.T))\n",
    "\t\t\n",
    "\t\treturn x_final, P_final\n",
    "\t\n",
    "\n",
    "\tdef run(self, x_init, P_init, u, y):\n",
    "\t\tx_est, P_est = self.predict(x_init, P_init, u)\n",
    "\t\treturn self.correct(x_est, P_est, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd611a",
   "metadata": {},
   "source": [
    "### 6.4 Extended Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a5b2c",
   "metadata": {},
   "source": [
    "#### 6.4.1 Initialization\n",
    "\n",
    "The constructor __init__ initializes the filter with necessary parameters:\n",
    "\n",
    "- `Q` : Process noise covariance matrix\n",
    "- `H` : Measurement matrix\n",
    "- `R` : Measurement noise covariance matrix\n",
    "- `sampling_rate` : The rate at which the measurements are sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186387dd",
   "metadata": {},
   "source": [
    "#### 6.4.2 Prediction Step\n",
    "\n",
    "The predict method implements the prediction step of the EKF.\n",
    "\n",
    "- `x_init`: Initial state estimate\n",
    "- `P_init` : Initial covariance estimate\n",
    "- `u` : Control input vector\n",
    "In this step, we calculate the estimated state x_est and the covariance of the estimate P_est using the state transition model and control input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81fff0",
   "metadata": {},
   "source": [
    "#### 6.4.3 Correction Step\n",
    "\n",
    "The correct method implements the correction step.\n",
    "\n",
    "- `x_est`: Estimated state from the prediction step\n",
    "- `P_est`: Estimated covariance from the prediction step\n",
    "- `y` : Measurement vector\n",
    "\n",
    "This step updates the estimated state and covariance based on the measurement y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3179d",
   "metadata": {},
   "source": [
    "#### 6.4.4 Running the Filter\n",
    "The run method combines the predict and correct steps to process a single measurement.\n",
    "\n",
    "`x_init`, `P_init`, `u`, `y`: Inputs for the prediction and correction steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c5880",
   "metadata": {},
   "source": [
    "## 7. Project Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd8ad8c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'serial'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pierr\\Documents\\Dev Projects\\MOBR\\Report.ipynb Cell 67\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pierr/Documents/Dev%20Projects/MOBR/Report.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmain\u001b[39;00m \u001b[39mimport\u001b[39;00m main\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pierr/Documents/Dev%20Projects/MOBR/Report.ipynb#Y120sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m Thymio\u001b[39m.\u001b[39mserial(port\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCOM9\u001b[39m\u001b[39m\"\u001b[39m, refreshing_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m) \u001b[39mas\u001b[39;00m th:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pierr/Documents/Dev%20Projects/MOBR/Report.ipynb#Y120sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \t\u001b[39mdir\u001b[39m(th)\n",
      "File \u001b[1;32mc:\\Users\\pierr\\Documents\\Dev Projects\\MOBR\\main.py:145\u001b[0m\n\u001b[0;32m    140\u001b[0m \t\t\ttime\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    141\u001b[0m \t\t\t\u001b[39m# cv2.imshow('background', background)\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \t\t\t\u001b[39m# cv2.imshow('camera', image)\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \t\t\t\u001b[39m# cv2.waitKey(int(1000/FPS))\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[39mwith\u001b[39;00m Thymio\u001b[39m.\u001b[39;49mserial(port\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCOM9\u001b[39;49m\u001b[39m\"\u001b[39;49m, refreshing_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m) \u001b[39mas\u001b[39;00m th:\n\u001b[0;32m    146\u001b[0m \t\u001b[39mdir\u001b[39m(th)\n\u001b[0;32m    148\u001b[0m \ttime\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pierr\\Documents\\Dev Projects\\MOBR\\Thymio.py:386\u001b[0m, in \u001b[0;36mThymio.serial\u001b[1;34m(port, node_id, refreshing_rate)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mserial\u001b[39m(port\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, node_id\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, refreshing_rate\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create Thymio object with a serial connection.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mserial\u001b[39;00m  \u001b[39m# pip3 install pyserial\u001b[39;00m\n\u001b[0;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m port \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m         port \u001b[39m=\u001b[39m Thymio\u001b[39m.\u001b[39mserial_default_port()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'serial'"
     ]
    }
   ],
   "source": [
    "from main import main\n",
    "\n",
    "with Thymio.serial(port=\"COM9\", refreshing_rate=0.1) as th:\n",
    "\tdir(th)\n",
    "\n",
    "\ttime.sleep(1)\n",
    "\n",
    "\tvariables = th.variable_description()\n",
    "\n",
    "\tfor var in variables:\n",
    "\t\tprint(var)\n",
    "\n",
    "\tmain(th)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
