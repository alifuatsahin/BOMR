{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0561116d",
   "metadata": {},
   "source": [
    "# Report : Project of Basics of Mobile Robotics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8acb39",
   "metadata": {},
   "source": [
    "Group members :\n",
    "\n",
    "- Cirillo Thomas\n",
    "- Muller Nathan\n",
    "- Sahin Ali Fuat\n",
    "- Vinet Pierre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9791982",
   "metadata": {},
   "source": [
    "## Introduction of the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617551",
   "metadata": {},
   "source": [
    "Our environment consist of a white surface with Aruco markers on the different corners to be able to identify them using the webcam given to us. We first decided to use a white surface with a black grid but then decided to go with a fully white surface instead because we use a binary threshold to detect our obstacles which would then interpret the grid as an obstacle. To correct this we would had needed to specifically extract the grid to create a mask out of it and substract it to our image in order to not detect it as an obstacle but we would not need this if we used a simple white surface. We also decided that the obstacle were not just squared one and therefor the use of a grid made less sense. \n",
    "\n",
    "The start and the goal are represented as Aruco markers as well. The obstacle used for the global navigation are black objects because as said before we use a binary threshold to detect them and blakc works the best for this situation, and the one used for the local avoidance are white objects because since they have the same color as the surface they are not detected with our image processing program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73394082",
   "metadata": {},
   "source": [
    "The code is structured into different parts :\n",
    "\n",
    "- Image Processing\n",
    "- Kalman Filter\n",
    "- Pathfinding\n",
    "- Global Navigation\n",
    "- Local Avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790079a",
   "metadata": {},
   "source": [
    "There is a different .py file for every part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d9673",
   "metadata": {},
   "source": [
    "## 1. Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c90eb",
   "metadata": {},
   "source": [
    "The goal of this section is to detect 4 aruco markers present at the 4 corners of the environment using the webcam given to use for the project. The reason we chose aruco markers is that we wanted markers tht the webcam could detect robustly. We tried using QR codes but we found out afterward that Aruco markers were more easily and consistently detected by the webcam than QR codes. Those were more sensible to light differences causing them to be less effectively detected than Aruco markers who seem more robust against those changes. Once the 4 corners are detected and the image has been correctly warped, it detects and updates the position of the robots and of the goal, thresholding the image to find the global obstacles and creating a grid for the pathinding algorithm to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51fc9a",
   "metadata": {},
   "source": [
    "Using the OpenCV library, we connect to the webcam using cv2.VideoCapture() and analyze the frames while the camera is connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the webcam \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#As long as the webcam is connected, the frames are store in the image variable\n",
    "while cap.isOpened():\n",
    "\t\tret, image = cap.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d95e31",
   "metadata": {},
   "source": [
    "Once the camera is connected, it will warp the image giving by the webcam to make the image fit the screen. To do so, it checks the transform_start variable : \n",
    "<ul>\n",
    "   <li> if set to <b>True</b>, it calls the functions detecting Aruco markers which return the different coordinates of the different markers\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17169bf",
   "metadata": {},
   "source": [
    "This aruco_read function (given below) takes as argument the image that will be processed and two boolean. \n",
    "It checks for Aruco markers on the image. When all the desired markers are detected, the coordinates and ids are written in a list used to draw their contours and center.\n",
    "\n",
    "It then returns this list and the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aruco_read(image, transform, start):\n",
    "\tARUCO_DICT = {\n",
    "\t\t\"DICT_4X4_50\": cv2.aruco.DICT_4X4_50,\n",
    "\t\t\"DICT_4X4_100\": cv2.aruco.DICT_4X4_100,\n",
    "\t\t\"DICT_4X4_250\": cv2.aruco.DICT_4X4_250,\n",
    "\t\t\"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n",
    "\t\t\"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,\n",
    "\t\t\"DICT_5X5_100\": cv2.aruco.DICT_5X5_100,\n",
    "\t\t\"DICT_5X5_250\": cv2.aruco.DICT_5X5_250,\n",
    "\t\t\"DICT_5X5_1000\": cv2.aruco.DICT_5X5_1000,\n",
    "\t\t\"DICT_6X6_50\": cv2.aruco.DICT_6X6_50,\n",
    "\t\t\"DICT_6X6_100\": cv2.aruco.DICT_6X6_100,\n",
    "\t\t\"DICT_6X6_250\": cv2.aruco.DICT_6X6_250,\n",
    "\t\t\"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n",
    "\t\t\"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,\n",
    "\t\t\"DICT_7X7_100\": cv2.aruco.DICT_7X7_100,\n",
    "\t\t\"DICT_7X7_250\": cv2.aruco.DICT_7X7_250,\n",
    "\t\t\"DICT_7X7_1000\": cv2.aruco.DICT_7X7_1000,\n",
    "\t\t\"DICT_ARUCO_ORIGINAL\": cv2.aruco.DICT_ARUCO_ORIGINAL,\n",
    "\t\t\"DICT_APRILTAG_16h5\": cv2.aruco.DICT_APRILTAG_16h5,\n",
    "\t\t\"DICT_APRILTAG_25h9\": cv2.aruco.DICT_APRILTAG_25h9,\n",
    "\t\t\"DICT_APRILTAG_36h10\": cv2.aruco.DICT_APRILTAG_36h10,\n",
    "\t\t\"DICT_APRILTAG_36h11\": cv2.aruco.DICT_APRILTAG_36h11\n",
    "\t}\n",
    "\n",
    "\taruco_dict = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[\"DICT_4X4_50\"])\n",
    "\taruco_params = cv2.aruco.DetectorParameters()\n",
    "\taruco_det = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)\n",
    "\n",
    "    #Detect if markers are recognized and stock the different variables in corners, ids and rejected\n",
    "\t(corners, ids, rejected) = aruco_det.detectMarkers(image)\n",
    "    \n",
    "    #Check if the ids detected from the markers (if any) are the correct one\n",
    "\tif ids is None:\n",
    "\t\treturn None, image\n",
    "\telif transform and 0 in ids and 1 in ids and 2 in ids and 3 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and start and 4 in ids and 5 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and not start and (4 in ids or 5 in ids):\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\treturn None, image\n",
    "    \n",
    "    #If all the correct markers are detected, put all their coordinates in a list and draw their contour and center\n",
    "\tcoord_list = []\n",
    "\tids = ids.flatten()\n",
    "    \n",
    "    #Goes through all the coordinates and ids of the detected markers\n",
    "\tfor (marker_corners, marker_ids) in zip(corners, ids):\n",
    "\t\tcorners = marker_corners.reshape((4,2))\n",
    "\t\t(topLeft, topRight, bottomRight, bottomLeft) = corners\n",
    "        \n",
    "        #Put all the coordinates and ids in a list\n",
    "\t\tcoord_list.append({'ID': marker_ids, 'POS': np.squeeze(corners)})\t\t\t\n",
    "\t\n",
    "        #Draw the contour of the marker\n",
    "\t\ttopRight = (int(topRight[0]), int(topRight[1]))\n",
    "\t\tbottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "\t\tbottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "\t\ttopLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    "\t\tcv2.line(image, topLeft, topRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, topRight, bottomRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomRight, bottomLeft, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomLeft, topLeft, (0, 255, 0), 2)\n",
    "\t\t\n",
    "        #Draw the center of it\n",
    "\t\tcX = int((topLeft[0] + bottomRight[0]) / 2.0)\n",
    "\t\tcY = int((topLeft[1] + bottomRight[1]) / 2.0)\n",
    "\t\tcv2.circle(image, (cX, cY), 4, (0, 0, 255), -1)\n",
    "\t\t\t\n",
    "\treturn coord_list, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8db3a",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Finally, if the tranform_start variable is set to <b>False</b>, it calls the function change_perspective whose goal is to warp the desired part of the image to make it fit the screen\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2159f",
   "metadata": {},
   "source": [
    "This function (given below) takes as arguments the image we want to warp, the different coordinates and ids of the Aruco markers and the size of the desired output.\n",
    "\n",
    "We decided to warp the image because it reduce the amount of unused data the program processes.\n",
    "\n",
    "The function store the coordinates of the different corners as input coordinates for the warp. It then declare output coordinates based on the size variable declared before.\n",
    "\n",
    "The way the warping works is that it takes input and output coordinates and warp the given image so that the input coordinate A is now situated at the output coordinate A (therefor, in our code it puts the different corners at the point (0,0), (1000,0), (0,1000), (1000,1000); 1000 being the value for size[0] and size[1])\n",
    "\n",
    "Finally, the function return the warped image to the main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_perpective(image, coords, size):\n",
    "\ttransform_coords = np.zeros((4,2))\n",
    "    \n",
    "    #Goes through all the detected markers to assignated corners coordinate as input coordinates\n",
    "\tfor el in coords:\n",
    "\t\tif el.get('ID') == 0:\n",
    "\t\t\ttransform_coords[0] = el.get('POS')[2]\n",
    "\t\telif el.get('ID') == 1:\n",
    "\t\t\ttransform_coords[1] = el.get('POS')[3]\n",
    "\t\telif el.get('ID') == 2:\n",
    "\t\t\ttransform_coords[2] = el.get('POS')[1]\n",
    "\t\telif el.get('ID') == 3:\n",
    "\t\t\ttransform_coords[3] = el.get('POS')[0]\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "    \n",
    "    #Declare corners (input coordinates) and dim (output coordinates)\n",
    "\tcorners = np.float32(transform_coords)\n",
    "\tdim = np.float32([[0,0],[size[0],0],[0,size[1]],[size[0],size[1]]])\n",
    "\n",
    "    #Warp the image\n",
    "\tperspective = cv2.getPerspectiveTransform(corners, dim)\n",
    "\timage = cv2.warpPerspective(image, perspective, (size[0], size[1]))\n",
    "\t\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e6420",
   "metadata": {},
   "source": [
    "Once the image has been warped, the same detecting process is done to detect the position of the robot and the goal, both detected with Aruco markers (the detection being made if the movement is starting and also if it has already being launched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2407d71",
   "metadata": {},
   "source": [
    "In order to detect the different obstacles once all of the previously stated elements are detected, a threshold is applied to the image. This threshold allows to better identify the global obstacles, the environment being white and the obstacles being black (the function is givenbelow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79727f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_threshold(image, border_size, robot_coords, goal_coords):\n",
    "    \n",
    "    #Convert to grayscale\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Reduce image noise\n",
    "\tblur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    \n",
    "    #Threshold image to better detect obstacle\n",
    "\t_, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\t\n",
    "    #Remove robot and goal Aruco marker from contour detections\n",
    "\tthresh = delete_aruco(thresh, robot_coords)\n",
    "\tthresh = delete_aruco(thresh, goal_coords)\n",
    "\n",
    "\tkernel = np.ones((5, 5), np.uint8) \n",
    "\tthresh = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    #Detect remaining obstacles\n",
    "\tcontours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcv2.drawContours(thresh, contours, -1, 0, border_size)\n",
    "\t\n",
    "\treturn thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c50eed",
   "metadata": {},
   "source": [
    "The image is first converted from BRG to grayscale to accuentuate the contrast between the elements. A gaussian blur is then applied to reduce the noise of the image improveing the obstacle detection. Finally, a threshold is applied to the image. We first only used a binary threshold because it was the most efficient one in our case (black obstacles with white surface) but the sensibility to light was to high so we added an Otsu threshold allowing to reduce the impact lightning had on the detection.\n",
    "\n",
    "The obstacles, that are polygons are detected with OpenCV. \n",
    "However, the Aruco markers used to represent the robot as well as the goal as also polygons, so, the delete_aruco function (given below) has to be called to not detect those two.\n",
    "\n",
    "The function then returns the thresholded image containing the obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f631f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_aruco(thresh, pos):\n",
    "\tmax_p = [0, 0]\n",
    "\tmin_p = [float('inf'), float('inf')]\n",
    "\tfor point in pos:\n",
    "\t\tif point[0] > max_p[0]:\n",
    "\t\t\tmax_p[0] = int(point[0])\n",
    "\t\tif point[0] < min_p[0]:\n",
    "\t\t\tmin_p[0] = int(point[0])\n",
    "\t\tif point[1] > max_p[1]:\n",
    "\t\t\tmax_p[1] = int(point[1])\n",
    "\t\tif point[1] < min_p[1]:\n",
    "\t\t\tmin_p[1] = int(point[1])\n",
    "\tthresh = cv2.rectangle(thresh, min_p, max_p, 255, -1)\n",
    "\treturn thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323a313",
   "metadata": {},
   "source": [
    "Finally, using the thresholded image containing the obstacles, a grid is created using the define_grid function (given below). This grid being used by the pathfinding algorithm, the spacing between each dot representing 80mm in rality. This spacing can be modified but this settings has be kept because it produces satisfying results.\n",
    "\n",
    "To create the grid, an empty matrix the size of the image is created and creates a dot everywhere there are no obstacles from the thresholded image.\n",
    "\n",
    "The function then returns the completed grid as well as the coordinates where there are no obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_grid(size, spacing, thresh):\n",
    "\tbackground = np.zeros([size[0], size[1], 1], dtype=np.uint8)\n",
    "\n",
    "\tvert = size[0]//spacing\n",
    "\thorz = size[1]//spacing\n",
    "\tv_blank = (size[0] % spacing)//2 + spacing\n",
    "\th_blank = (size[1] % spacing)//2 + spacing\n",
    "\n",
    "\tcoord_init = (v_blank, h_blank)\n",
    "\tgrid = []\n",
    "\tcoord = []\n",
    "\n",
    "\tfor h in range(horz-1):\n",
    "\t\tcoord_init = (h_blank + h*spacing, v_blank)\n",
    "\t\tfor v in range(vert-1):\n",
    "\t\t\tif thresh.T[coord_init] == 255:\n",
    "\t\t\t\tbackground = cv2.circle(background, coord_init, 5, 255, -1)\n",
    "\t\t\t\tgrid.append((h,v))\n",
    "\t\t\t\tcoord.append(coord_init)\n",
    "\t\t\tcoord_init = (coord_init[0], coord_init[1] + spacing)\n",
    "\t\t\t\n",
    "\treturn grid, coord, background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f65832",
   "metadata": {},
   "source": [
    "Finally, a find_pos function is called in the main, returning the corresponding position of the start and the goal on the grid we just created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
