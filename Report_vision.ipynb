{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0561116d",
   "metadata": {},
   "source": [
    "# Report : Project of Basics of Mobile Robotics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8acb39",
   "metadata": {},
   "source": [
    "Group members :\n",
    "\n",
    "- Cirillo Thomas\n",
    "- Muller Nathan\n",
    "- Sahin Ali Fuat\n",
    "- Vinet Pierre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9791982",
   "metadata": {},
   "source": [
    "## Introduction of the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617551",
   "metadata": {},
   "source": [
    "Our environment consist of a white surface with Aruco markers on the different corners to be able to identify them using the webcam given to us. We first decided to use a white surface with a black grid but then decided to go with a fully white surface instead because we use a binary threshold to detect our obstacles which would then interpret the grid as an obstacle. To correct this we would had needed to specifically extract the grid to create a mask out of it and substract it to our image in order to not detect it as an obstacle but we would not need this if we used a simple white surface. We also decided that the obstacle were not just squared one and therefor the use of a grid made less sense. \n",
    "\n",
    "The start and the goal are represented as Aruco markers as well. The obstacle used for the global navigation are black objects because as said before we use a binary threshold to detect them and blakc works the best for this situation, and the one used for the local avoidance are white objects because since they have the same color as the surface they are not detected with our image processing program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73394082",
   "metadata": {},
   "source": [
    "The code is structured into different parts :\n",
    "\n",
    "- Image Processing\n",
    "- Kalman Filter\n",
    "- Pathfinding\n",
    "- Global Navigation\n",
    "- Local Avoidance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790079a",
   "metadata": {},
   "source": [
    "There is a different .py file for every part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d9673",
   "metadata": {},
   "source": [
    "## 1. Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c90eb",
   "metadata": {},
   "source": [
    "The goal of this section is to detect 4 aruco markers present at the 4 corners of the environment using the webcam given to use for the project. The reason we chose aruco markers is that we wanted markers tht the webcam could detect robustly. We tried using QR codes but we found out afterward that Aruco markers were more easily and consistently detected by the webcam than QR codes. Those were more sensible to light differences causing them to be less effectively detected than Aruco markers who seem more robust against those changes. Once the 4 corners are detected and the image has been correctly warped, it detects and updates the position of the robots and of the goal, thresholding the image to find the global obstacles and creating a grid for the pathinding algorithm to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51fc9a",
   "metadata": {},
   "source": [
    "Using the library OpenCV, we connect to the webcam and keep on taking frames to analyze while the camera is connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0f6e7",
   "metadata": {},
   "source": [
    "```python\n",
    "#Import the OpenCV library to use the different modules\n",
    "import cv2\n",
    "\n",
    "#Connect to the webcam \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#As long as the webcam is connected, the frames are store in the image variable\n",
    "while cap.isOpened():\n",
    "\t\tret, image = cap.read()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d2887",
   "metadata": {},
   "source": [
    "Once the camera is connected, it will warp the image giving by the webcam to make the image fit the screen. To do so, it checks the transform_start variable : \n",
    "<ul>\n",
    "   <li> if set to <b>True</b>, it calls the functions detecting Aruco markers which return the different coordinates of the different markers\n",
    "   \n",
    "```python\n",
    "    if transform_start:\n",
    "                (coords, image) = aruco_read(image, transform=True, start=True)\n",
    "```\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17169bf",
   "metadata": {},
   "source": [
    "This aruco_read function (given below) takes as argument the image that will be processed as well as two boolean variables (transform and start). \n",
    "It checks for Aruco markers on the image. An if branchement make sure that if all the wanted markers are not visible, it returns an empty array instead of coordinate as well as the original image.\n",
    "Finally, if all the different markers are detected, it put their different coordinate in a list, draw 4 lines around it to make it more visible on the image and draw a red dot in its center. After it has gone through all the markers, it return the list containing the coordinates and the image with the drawn line and dots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda3db4",
   "metadata": {},
   "source": [
    "```python\n",
    "def aruco_read(image, transform, start):\n",
    "\tARUCO_DICT = {\n",
    "\t\t\"DICT_4X4_50\": cv2.aruco.DICT_4X4_50,\n",
    "\t\t\"DICT_4X4_100\": cv2.aruco.DICT_4X4_100,\n",
    "\t\t\"DICT_4X4_250\": cv2.aruco.DICT_4X4_250,\n",
    "\t\t\"DICT_4X4_1000\": cv2.aruco.DICT_4X4_1000,\n",
    "\t\t\"DICT_5X5_50\": cv2.aruco.DICT_5X5_50,\n",
    "\t\t\"DICT_5X5_100\": cv2.aruco.DICT_5X5_100,\n",
    "\t\t\"DICT_5X5_250\": cv2.aruco.DICT_5X5_250,\n",
    "\t\t\"DICT_5X5_1000\": cv2.aruco.DICT_5X5_1000,\n",
    "\t\t\"DICT_6X6_50\": cv2.aruco.DICT_6X6_50,\n",
    "\t\t\"DICT_6X6_100\": cv2.aruco.DICT_6X6_100,\n",
    "\t\t\"DICT_6X6_250\": cv2.aruco.DICT_6X6_250,\n",
    "\t\t\"DICT_6X6_1000\": cv2.aruco.DICT_6X6_1000,\n",
    "\t\t\"DICT_7X7_50\": cv2.aruco.DICT_7X7_50,\n",
    "\t\t\"DICT_7X7_100\": cv2.aruco.DICT_7X7_100,\n",
    "\t\t\"DICT_7X7_250\": cv2.aruco.DICT_7X7_250,\n",
    "\t\t\"DICT_7X7_1000\": cv2.aruco.DICT_7X7_1000,\n",
    "\t\t\"DICT_ARUCO_ORIGINAL\": cv2.aruco.DICT_ARUCO_ORIGINAL,\n",
    "\t\t\"DICT_APRILTAG_16h5\": cv2.aruco.DICT_APRILTAG_16h5,\n",
    "\t\t\"DICT_APRILTAG_25h9\": cv2.aruco.DICT_APRILTAG_25h9,\n",
    "\t\t\"DICT_APRILTAG_36h10\": cv2.aruco.DICT_APRILTAG_36h10,\n",
    "\t\t\"DICT_APRILTAG_36h11\": cv2.aruco.DICT_APRILTAG_36h11\n",
    "\t}\n",
    "\n",
    "\taruco_dict = cv2.aruco.getPredefinedDictionary(ARUCO_DICT[\"DICT_4X4_50\"])\n",
    "\taruco_params = cv2.aruco.DetectorParameters()\n",
    "\taruco_det = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)\n",
    "\n",
    "    #Detect if markers are recognized and stock the different variables in corners, ids and rejected\n",
    "\t(corners, ids, rejected) = aruco_det.detectMarkers(image)\n",
    "    \n",
    "    #Check if the ids detected from the markers (if any) are the correct one\n",
    "\tif ids is None:\n",
    "\t\treturn None, image\n",
    "\telif transform and 0 in ids and 1 in ids and 2 in ids and 3 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and start and 4 in ids and 5 in ids:\n",
    "\t\tpass\n",
    "\telif not transform and not start and (4 in ids or 5 in ids):\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\treturn None, image\n",
    "    \n",
    "    #If all the correct markers are detected, put all their coordinates in a list and draw their contour and center\n",
    "\tcoord_list = []\n",
    "\tids = ids.flatten()\n",
    "    \n",
    "    #Goes through all the coordinates and ids of the detected markers\n",
    "\tfor (marker_corners, marker_ids) in zip(corners, ids):\n",
    "\t\tcorners = marker_corners.reshape((4,2))\n",
    "\t\t(topLeft, topRight, bottomRight, bottomLeft) = corners\n",
    "        \n",
    "        #Put all the coordinates and ids in a list\n",
    "\t\tcoord_list.append({'ID': marker_ids, 'POS': np.squeeze(corners)})\t\t\t\n",
    "\t\n",
    "        #Draw the contour of the marker\n",
    "\t\ttopRight = (int(topRight[0]), int(topRight[1]))\n",
    "\t\tbottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "\t\tbottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "\t\ttopLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    "\t\tcv2.line(image, topLeft, topRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, topRight, bottomRight, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomRight, bottomLeft, (0, 255, 0), 2)\n",
    "\t\tcv2.line(image, bottomLeft, topLeft, (0, 255, 0), 2)\n",
    "\t\t\n",
    "        #Draw the center of it\n",
    "\t\tcX = int((topLeft[0] + bottomRight[0]) / 2.0)\n",
    "\t\tcY = int((topLeft[1] + bottomRight[1]) / 2.0)\n",
    "\t\tcv2.circle(image, (cX, cY), 4, (0, 0, 255), -1)\n",
    "\t\t\t\n",
    "\treturn coord_list, image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafda1f5",
   "metadata": {},
   "source": [
    "Once this function has return the different coordinates and image to the main, it checks if if the coordinates vector is empty or not and if ny coordinates are given, it switch transform_start to <b>False</b>:\n",
    "\n",
    "```python\n",
    "if coords is not None:\n",
    "    transform_start = False\n",
    "continue         \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96757f",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Finally, if the tranform_start variable is set to <b>False</b>, it calls the function change_perspective whoe goal is to warp the desired part of the image to make it fit the screen\n",
    "</ul>\n",
    "\n",
    "```python\n",
    "else:\n",
    "\timage = change_perpective(image, coords, size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922dc55",
   "metadata": {},
   "source": [
    "This function (given below) takes as arguments the image we want to warp, the different coordinates and ids of the Aruco markers we detected previously and the size of the desired output. Going through all the markers detected, an if branchement checks if the procesed marker is on of the corners and put it as entry coordinates for the warp. After this, it put all the given entry coordinates in a corner variable (transforming them in float by the same occasion). It does the same for the output coordinates putting them in a dim variable (also transforming them in float).\n",
    "The getPerspectiveTransform and warpPerspective functions from Open CV are called.\n",
    "The way the warping works is that it takes input and output coordinates and warp the given image so that the input coordinate A is now situated at the output coordinate A (therefor, in our code it puts the different corners at the point (0,0), (1000,0), (0,1000), (1000,1000); 1000 being the value for size[0] and size[1])\n",
    "Finally, the function return the warped image to the main."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32894a",
   "metadata": {},
   "source": [
    "```python\n",
    "def change_perpective(image, coords, size):\n",
    "\ttransform_coords = np.zeros((4,2))\n",
    "    \n",
    "    #Goes through all the detected markers to assignated corners coordinate as input coordinates\n",
    "\tfor el in coords:\n",
    "\t\tif el.get('ID') == 0:\n",
    "\t\t\ttransform_coords[0] = el.get('POS')[2]\n",
    "\t\telif el.get('ID') == 1:\n",
    "\t\t\ttransform_coords[1] = el.get('POS')[3]\n",
    "\t\telif el.get('ID') == 2:\n",
    "\t\t\ttransform_coords[2] = el.get('POS')[1]\n",
    "\t\telif el.get('ID') == 3:\n",
    "\t\t\ttransform_coords[3] = el.get('POS')[0]\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "    \n",
    "    #Declare corners (input coordinates) and dim (output coordinates)\n",
    "\tcorners = np.float32(transform_coords)\n",
    "\tdim = np.float32([[0,0],[size[0],0],[0,size[1]],[size[0],size[1]]])\n",
    "\n",
    "    #Warp the image\n",
    "\tperspective = cv2.getPerspectiveTransform(corners, dim)\n",
    "\timage = cv2.warpPerspective(image, perspective, (size[0], size[1]))\n",
    "\t\n",
    "\treturn image\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b0b64",
   "metadata": {},
   "source": [
    "Once the image has been warped, the same detecting process is done to detect the position of the robot and the goal, both detected with Aruco markers (the detection being made if the movement is starting and also if it has already being launched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0cd484",
   "metadata": {},
   "source": [
    "For the goal, we want to be able to find its precise position, so we use the calculate_centroid function. This function (given below) takes as argument coordinates of the Aruco markers we want to find the center of (in our case the coordinates from the goal). It then calculate its center and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c96f7",
   "metadata": {},
   "source": [
    "```python\n",
    "def calculate_centroid(coords):\n",
    "    x_center = 0\n",
    "    y_center = 0\n",
    "    for i in range(4):\n",
    "        x_center += coords[i][0]/4\n",
    "        y_center += coords[i][1]/4\n",
    "    return [x_center, y_center]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2407d71",
   "metadata": {},
   "source": [
    "In order to detect the different obstacles once all of the previously stated elements are detected, a threshold is applied to the image. This threshold allows to better identify the global obstacles, the environment being white and the obstacles being black (the function is givenbelow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43172889",
   "metadata": {},
   "source": [
    "```python\n",
    "def image_threshold(image, border_size, robot_coords, goal_coords):\n",
    "    \n",
    "    #Convert to grayscale\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Reduce image noise\n",
    "\tblur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    \n",
    "    #Threshold image to better detect obstacle\n",
    "\t_, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\t\n",
    "    #Remove robot and goal Aruco marker from contour detections\n",
    "\tthresh = delete_aruco(thresh, robot_coords)\n",
    "\tthresh = delete_aruco(thresh, goal_coords)\n",
    "\n",
    "\tkernel = np.ones((5, 5), np.uint8) \n",
    "\tthresh = cv2.dilate(thresh, kernel, iterations=2)\n",
    "\n",
    "    #Detect remaining obstacles\n",
    "\tcontours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcv2.drawContours(thresh, contours, -1, 0, border_size)\n",
    "\t\n",
    "\treturn thresh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c50eed",
   "metadata": {},
   "source": [
    "As we can see above, the image is first converted from BRG to grayscale in order to accuentuate the contrast between the elements. A gaussian blur is then applied, his goal being to reduce the noise of the image to improve the obstacle detection. Finally, a threshold is applied to the image. We first only used a binary threshold but the resulted algorithm was very dependent on the light conditions and we therefor added an Otsu threshold which allowed to reduce the impact lightning had on the detection.\n",
    "\n",
    "The obstacle detection uses contour detection checking for any polygon it can find on the image. However, the Aruco markers used to represent the robot as well as the goal as also polygons, so, the delete_aruco function (given below) is called, in order to ignore those two.\n",
    "\n",
    "```python\n",
    "def delete_aruco(thresh, pos):\n",
    "\tmax_p = [0, 0]\n",
    "\tmin_p = [float('inf'), float('inf')]\n",
    "\tfor point in pos:\n",
    "\t\tif point[0] > max_p[0]:\n",
    "\t\t\tmax_p[0] = int(point[0])\n",
    "\t\tif point[0] < min_p[0]:\n",
    "\t\t\tmin_p[0] = int(point[0])\n",
    "\t\tif point[1] > max_p[1]:\n",
    "\t\t\tmax_p[1] = int(point[1])\n",
    "\t\tif point[1] < min_p[1]:\n",
    "\t\t\tmin_p[1] = int(point[1])\n",
    "\tthresh = cv2.rectangle(thresh, min_p, max_p, 255, -1)\n",
    "\treturn thresh\n",
    "```\n",
    "\n",
    "Then, we use the findContours and drawContours functions from Open CV to detect the global obstacles.\n",
    "\n",
    "The function returning the thresholded image containing only the obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d1f0c",
   "metadata": {},
   "source": [
    "Finally, using the thresholded image containing the obstacles, a grid is created using the define_grid function (given below). This grid being used by the pathfinding algorithm.\n",
    "\n",
    "```python\n",
    "def define_grid(size, spacing, thresh):\n",
    "\tbackground = np.zeros([size[0], size[1], 1], dtype=np.uint8)\n",
    "\n",
    "\tvert = size[0]//spacing\n",
    "\thorz = size[1]//spacing\n",
    "\tv_blank = (size[0] % spacing)//2 + spacing\n",
    "\th_blank = (size[1] % spacing)//2 + spacing\n",
    "\n",
    "\tcoord_init = (v_blank, h_blank)\n",
    "\tgrid = []\n",
    "\tcoord = []\n",
    "\n",
    "\tfor h in range(horz-1):\n",
    "\t\tcoord_init = (h_blank + h*spacing, v_blank)\n",
    "\t\tfor v in range(vert-1):\n",
    "\t\t\tif thresh.T[coord_init] == 255:\n",
    "\t\t\t\tbackground = cv2.circle(background, coord_init, 5, 255, -1)\n",
    "\t\t\t\tgrid.append((h,v))\n",
    "\t\t\t\tcoord.append(coord_init)\n",
    "\t\t\tcoord_init = (coord_init[0], coord_init[1] + spacing)\n",
    "\t\t\t\n",
    "\treturn grid, coord, background\n",
    "```\n",
    "\n",
    "It first created a zero filled matrix of the size of the image. An arbitrary spacing is creating so that the different point of the grid are separated by the same distance. A double for loop then goes through the entire matrix and check the value of the thresholded image obtained before at the position we are verifying. If the value is 255 (white), a white dot is put at the position meaning it is free of any obstacles and the coordinates are put in a list containing all the obstacle free coordinate.\n",
    "\n",
    "The function then returns the grid, the completed background as well as the coordinates where there are no obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d0005",
   "metadata": {},
   "source": [
    "```python\n",
    "def find_pos(pos, grid, coord):\n",
    "\tgrid_c = None\n",
    "\tdist = float('inf')\n",
    "\tfor point in coord:\n",
    "\t\ttemp = euclidean_distance(point, pos)\n",
    "\t\tif temp < dist:\n",
    "\t\t\tgrid_c = grid[coord.index(point)]\n",
    "\t\t\tdist = temp\n",
    "\n",
    "\treturn grid_c\n",
    "```\n",
    "\n",
    "Finally, the function find_pos (given above) is called in the main, returning the corresponding position of the start and the goal on the grid we just created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
